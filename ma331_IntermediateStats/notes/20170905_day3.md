## Sampling distribution: Central Limit Theorem (CLT)
* We only ever have a sample from the population
    * We can't know the actual mean of the population, we use the sample to estimate
    * Sample mean estimates population mean \(\overline(X)= \frac{1}{h}\sum^{n}_{i=1}{X^i}\)
    * *Variance* of the sample mean tells us the *spread* of it
* Some issues
    1. Accuracy
    2. \(\overline{X}-\mu\) distribution
    3. \(x_1=\chi_1\, ..., x_n=\chi_n, \overline{X}=\overline{\chi}\)

## Three pillars of statistics
### \(\chi^2\) distribution
* A collection of independent random samples, we sum the squares of the values, then \(X=\sum^{n}_{i=1}{X^2_i}\) is said to be the \(\chi^2_n\) distribution with degree of freedom(df) n and denoted as \(X\) ~ \(\chi^2_n\)
* It can't be negative because we square all the values, the the probability of getting a negative value is 0

1. \(E[X+Y]=E[X] + E[Y]\)
2. For X, \(Var[X]=E[X^2]-E^2[X]\)
3. X~N(0,1), \(E[X]=\mu, Var[X]=\sigma^2\)
4. If *X and Y are independent*, then \(Var[X+Y]=Var[X] + Var[Y]\)
5. For X~f(density function f) and h(x), \(E[h(x)]=\int^{\inf}_{-\inf}{h(x)f(x)dx}\)

* The **degree of freedom** of a \(\chi^2\) distribution can be thought of as the mean of the random variable

### Student's \(t\) distribution
* X ~ N(0,1) is independent of Y~\(\chi^2_n\), then \(T=\frac{X}{\sqrt{\frac{Y}{n}}}\) is said to be of \(t_n\) distribution with df n, and denoted as T ~ \(t_n\)
* A \(t\) distribution converges to the standard normal distribution as the sample size increases

### \(F\) distribution
* X~\(\chi^2_n\) and Y~\(\chi^2_m\) are mutually independent, then \(F=\frac{X/n}{Y/m}\) is said to be of \(F_{n,m}\) distribution with df (n,m), denoted as F~\(F_{n,m}\)
